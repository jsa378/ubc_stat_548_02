% !TEX root = ../main.tex

% Project report section

\section{Project Report}

\subsection{Project Description}

\subsubsection{Problems With the Proposed Project}

The proposed project concerned an application of Gaussian processes on graphs to Markov decision processes, the latter being the standard framework for reinforcement learning. The general idea was that a reinforcement learning agent would be tasked with estimating shortest paths on a weighted graph, with only partially known and perhaps evolving weights.

However, it was never precisely clear what type of Markov decision process to choose for this problem and how to define its components. For example, since the graph weights were only partially known, it might make sense to use a partially observable Markov decision process, but it was not clear how to precisely define the partially observable Markov decision process. In an email dated November 2\textsuperscript{nd}, Professor Pleiss suggested a somewhat different setup in which the costs on the graph edges are known, but the rewards on the graph vertices are unknown. This suggestion was presumably made to conform with the fact that the graph Mat\'{e}rn Gaussian process described in \cite{pmlr-v130-borovitskiy21a} is define on the graph vertices, not its edges, but this setup is no longer obviously a partially observable Markov decision process.

\subsubsection{A New Project Direction: Estimating Shortest Paths}

We decided to abandon the proposed connection to reinforcement learning and re-frame the project in terms of graph theory and Gaussian processes. More precisely, we consider the following problem. Suppose we have a weighted graph $G = (V, E)$ with weight function $w \colon E \to \mathbb R$, and only a subset of the weights are known. Mathematically, we can suppose there is some strict subset $E' \subset E$, and only the restriction $w \restriction E' \colon E' \to \mathbb R$ is known. We would like to estimate the entire weight function $w \colon E \to \mathbb R$, via a Gaussian process. This estimate of $w$ might be written $\hat w$. We would like that $\hat w \restriction E' = w \restriction E'$, and that the values of $\hat w$ over the unobserved weights on edges $E \setminus E'$ are estimated via a Gaussian process.

In particular, we would like to develop estimates of the unobserved weights in order to compute \textit{estimated shortest paths} on the graph $G$. Given estimates of the unobserved weights, estimated shortest paths are calculated by the application of a shortest path algorithm, for example Dijkstra's algorithm, on what we might call the graph $\hat G$, an edge-weighted graph with weight function $\hat w$. Informally, the graph $\hat G$ is $G$, but with the edge weights that we had not observed filled in with our estimates of those weights.

Given this new project direction, we must consider the following issues:

\begin{enumerate}
    \item how to formulate the shortest path estimation mathematically,
    \item how to implement the shortest path estimation computationally,
    \item how to evaluate the performance of a method for estimating shortest paths.
\end{enumerate}

\subsection{Mathematical Formulation}

An initial stumbling block is that we wish to learn $w \colon E \to \mathbb R$, but the graph Mat\'{e}rn Gaussian process as defined in \cite{pmlr-v130-borovitskiy21a} is defined on the \textit{vertices} of a graph, not its edges. (It would be helpful to understand why we cannot define a Gaussian process on the edges of a weighted graph, since this is would be the na\"{i}ve choice.)

Given this limitation, we would like to adapt a edge-weighted graph so that its edges become vertices, in some sense. There at at least two ways to do this, which will be described below.

\subsubsection{First Method: Add Extra Vertices to \texorpdfstring{$G$}{G}}

Given a weighted graph $G_1 = (V, E)$, one possible approach to adapt $G_1$ appears to be to add vertices to $G_1$ on the existing edges, thereby splitting each edge into two edges. A simple example is shown in Figures \ref{fig:add_nodes_1} and \ref{fig:add_nodes_2}. Beginning in Figure \ref{fig:add_nodes_1} with a graph $G_1$, this graph is then transformed into a graph $G_2$ in Figure \ref{fig:add_nodes_2}.

This appears to be roughly the approach followed in one of the experiments described in Appendix A of \cite{pmlr-v130-borovitskiy21a}, as suggested by the following sentence from the aforementioned Appendix:

\begin{quote}
    We bind the traffic congestion data to the graph by adding additional nodes that subdivide existing edges of the graph at the location of the measurement points.
\end{quote}

One apparent issue with this method is that it's not clear how to determine the edge weights for the new edges. It's also unclear how much of the original structure of $G_1$ is preserved by $G_2$.

\begin{figure}
    \begin{center}
        \begin{tikzpicture}
            \begin{scope}[every node/.style={circle,thick,draw}]
                \node (A) at (0,0) {\textbf A};
                \node (B) at (0,4) {\textbf B};
                \node (C) at (4,4) {\textbf C};
                \node (D) at (4,0) {\textbf D};
            \end{scope}
            
            \begin{scope}[>={Stealth[black]},
                          every node/.style={fill=white,circle},
                          every edge/.style={draw=black,very thick}]
                \path [-] (A) edge node {$\mathbf w_1$} (B);
                \path [-] (B) edge node {$\mathbf w_2$} (C);
                \path [-] (C) edge node {$\mathbf w_3$} (D);
                \path [-] (A) edge node {$\mathbf w_4$} (D);
            \end{scope}
            \end{tikzpicture}
    \end{center}
    \caption{This figure shows a simple edge-weighted graph $G_1$. If only some subset of the weights $\left\{ \mathbf w_1, \mathbf w_2, \mathbf w_3, \mathbf w_4 \right\}$ are known, we would like to use a Gaussian process in order to estimate the unknown weights. However, because the graph Mat\'{e}rn Gaussian process is defined on the \textit{vertices} of an edge-weighted graph, it is necessary to turn the edges of $G_1$ into nodes, in some manner.}
    \label{fig:add_nodes_1}
\end{figure}

\begin{figure}
    \begin{center}
        \begin{tikzpicture}
            \begin{scope}[every node/.style={circle,thick,draw}]
                \node (A) at (0,0) {\textbf A};
                \node (B) at (0,4) {\textbf B};
                \node (C) at (4,4) {\textbf C};
                \node (D) at (4,0) {\textbf D};
                \node (w1) at (0,2) {$\mathbf w_1$};
                \node (w2) at (2,4) {$\mathbf w_2$};
                \node (w3) at (4,2) {$\mathbf w_3$};
                \node (w4) at (2,0) {$\mathbf w_4$};
            \end{scope}
            
            \begin{scope}[>={Stealth[black]},
                          every node/.style={fill=white,circle},
                          every edge/.style={draw=black,very thick}]
                \path [-] (A) edge node {$\mathbf w_1'$} (w1);
                \path [-] (w1) edge node {$\mathbf w_2'$} (B);
                \path [-] (B) edge node {$\mathbf w_3'$} (w2);
                \path [-] (w2) edge node {$\mathbf w_4'$} (C);
                \path [-] (C) edge node {$\mathbf w_5'$} (w3);
                \path [-] (w3) edge node {$\mathbf w_6'$} (D);
                \path [-] (D) edge node {$\mathbf w_7'$} (w4);
                \path [-] (w4) edge node {$\mathbf w_8'$} (A);
            \end{scope}
            \end{tikzpicture}
    \end{center}
    \caption{This figure shows one possible method to convert the graph $G_1$ from Figure \ref{fig:add_nodes_1} into a graph $G_2$ suitable for a graph Mat\'{e}rn Gaussian process. In graph $G_2$, we have inserted new vertices on each edge of $G_1$ and allowed each new vertex to inherit the appropriate edge weight. However, it is unclear how the new weights on the new, shortened vertices should be determined.}
    \label{fig:add_nodes_2}
\end{figure}

\subsubsection{Second Method: Convert \texorpdfstring{$G$}{G} Into Its Line Graph \texorpdfstring{$L(G)$}{LofG}}

Informally, the line graph $L(G_1)$ of a graph $G_1$ is a graph such that the edges of $G_1$ are vertices of $L(G_1)$, while the vertices of $G_1$ are edges of $L(G_1)$. (This is an over-simplification---one can consult \href{https://en.wikipedia.org/wiki/Line_graph}{Wikipedia}, or any number of textbooks on graph theory, such as \cite{moderngraph}, for more details.) The second approach is to form the line graph of $G_1$ and then apply the graph Mat\'{e}rn Gaussian process to $L(G_1)$. A simple example is shown in Figure \ref{fig:add_nodes_3}.

For the line graph approach, similar to the first method, it is also unclear how to determine the new edge weights. It's also unclear how much of the original structure of $G_1$ is preserved by $L(G_1)$.

\begin{figure}
    \begin{center}
        \begin{tikzpicture}
            \begin{scope}[every node/.style={circle,thick,draw}]
                \node (w1) at (0,2) {$\mathbf w_1$};
                \node (w2) at (2,4) {$\mathbf w_2$};
                \node (w3) at (4,2) {$\mathbf w_3$};
                \node (w4) at (2,0) {$\mathbf w_4$};
            \end{scope}
            
            \begin{scope}[>={Stealth[black]},
                          every node/.style={fill=white,circle},
                          every edge/.style={draw=black,very thick}]
                \path [-] (w1) edge node {$\mathbf w_1''$} (w2);
                \path [-] (w2) edge node {$\mathbf w_2''$} (w3);
                \path [-] (w3) edge node {$\mathbf w_3''$} (w4);
                \path [-] (w4) edge node {$\mathbf w_4''$} (w1);
            \end{scope}
            \end{tikzpicture}
    \end{center}
    \caption{This figure shows another possible method to convert the graph $G_1$ from Figure \ref{fig:add_nodes_1} into a graph $G_3$ suitable for a graph Mat\'{e}rn Gaussian proecess. The graph $G_3$ is the line graph of $G_1$. One possible issue with this method is that for more complicated graphs, it's not clear how much of the original structure of $G_1$ is preserved by $G_3$. The graph Mat\'{e}rn Gaussian process will be applied on $G_3$, but the inferences made are ultimately only of interest in the context of the unknown edge weights on graph $G_1$.}
    \label{fig:add_nodes_3}
\end{figure}

\subsubsection{Discussion of Both Methods}

Without much experience with graphs, it is difficult to have much intuition for the advantages and disadvantages of each of the two methods described above. Presumably, both should be explored. However, the following observations can be made:

\begin{enumerate}
    \item Since the graph Mat\'{e}rn Gaussian process will be applied to either $G_2$ or $G_3$, when the actual graph of interest is $G_1$, presumably it is important that the graph to which the process is applied be as structurally similar to $G_1$ as possible, in order to maximize the accuracy of the estimates of the unknown weights of $G_1$. The line graph $G_3$ appears much more similar to $G_1$ than does $G_2$, but this is a very simple example, and does not necessarily generalize.
    \item It seems apparent that the first method, producing the graph $G_2$, results in a graph that is much bigger than graph $G_3$. In graph $G_2$, the number of nodes and weights have both doubled compared to $G_1$, whereas the line graph $G_3$ is the same size a the original graph $G_1$. One would suspect that this might have ramifications for the computational feasibility of the two approaches when the original graph $G_1$ is large.
\end{enumerate}

% The graphs in Figures \ref{fig:add_nodes_1}--\ref{fig:add_nodes_3} are admittedly extremely simple examples, but 

\subsubsection{Which Edge Weights Should be Initially Observed?}

A question that is probably important to consider is \textit{which} edge weights are initially known. For example, one could assume that any random subset of the edge weights could be known initially, or one could assume that only ``contiguous'' subsets of the edge weights are initially known.

This issue is presumably related to the discussion of computational implementation in Section \ref{sec:comp_implem}. It is also closely related to potential applications, which are discussed in Section \ref{sec:pot_app}.

\subsubsection{Exact Form of the Covariance Matrix}

Another point of confusion in the mathematical formulation of the project is the precise form of the covariance matrix of the graph Mat\'{e}rn Gaussian process, given some observed data. The covariance function (and the related matrix) of a Gaussian process is perhaps its most important feature, so it is critical to have a solid understanding of this aspect of the project. (I am omitting discussion of the derivation of the graph Mat\'{e}rn Gaussian process itself, which is not clear either. Presumably understanding this would also be helpful.)

There is a brief discussion of the covariance matrix in Appendix B of \cite{pmlr-v130-borovitskiy21a}, which appears to proceed as follows. Given an edge-weighted graph $G = (V, E)$ with non-negative weights, the graph Laplacian $\bm \Delta$ of $G$ is a symmetric, positive semi-definite matrix admitting an eigen-decomposition $\bm \Delta = \mathbf U \bm \Lambda \mathbf U^\intercal$. Given some subset of vertices $\bm x \in V^n$, we define $\mathbf U(\bm x)$ to be the $n \times d$ submatrix of $\mathbf U$ with rows corresponding to the elements of $\bm x$. Then it appears that the covariance matrix $\mathbf K_{\bm {xx}}$ of the graph Mat\'{e}rn Gaussian process is given by

\[
    \mathbf K_{\bm{xx}} = \mathbf U(\bm x)\Phi(\bm \Lambda)^{-2} \mathbf U^\intercal,
\]

where $\Phi$ is as defined in (\ref{eq:Phi_1}). However, it is not clear how this result is derived, or whether it is the general form of a covariance matrix for a graph Mat\'{e}rn Gaussian process. It is similarly unclear how this result should actually be used. Is $\bm x$ to be interpreted as the data, i.e. the edge weights of $G$, that were actually observed? This is unclear.

\subsection{Computational Implementation}\label{sec:comp_implem}

It is not clear how Gaussian process training would proceed. For example, given an edge-weighted graph $G$ where some subset of the edge weights are known, and another graph $G'$ that is in a suitable form of the graph Mat\'{e}rn Gaussian process, is the prior Gaussian process on $G'$ updated to the posterior (given the observed data) one time, or is this done in a more iterative fashion? Perhaps this will depend on the characteristics of the original graph $G$ and how the data are gathered. If the data are gathered in one shot, then perhaps we will only form the posterior process one time, but if the data are gathered incrementally, or the observed weights are changing over time, then presumably we will need to form the posterior process multiple times.

\subsection{Performance Evaluation}

Suppose we have an edge-weighted graph $G = (V, E)$, and we have observed the weights on some strict subset $E' \subset E$. Furthermore, suppose we have used a graph Mat\'{e}rn Gaussian process to estimate the values of the unobserved weights on the remaining edges $E \setminus E'$. It is important to develop some method of evaluating the quality of our weight estimations. There seem to be at least a few possible approaches.

First, we can use some sort of loss function, e.g. squared loss, by comparing the estimated weights with their true values. For example, if a graph $G$ has $N$ edge weights, and $1 < n < N$ of these weights are observed, then we can form the estimates $\hat w_{n + 1}, \hat w_{n + 2}, \dots, \hat w_N$ and then compute the loss

\begin{equation}\label{eq:loss_1}
    \sum_{i = n + 1}^N \left(w_i - \hat w_i\right)^2.
\end{equation}

In a somewhat similar vein, we might be interested in confidence intervals concerning our estimates of the unknown weights. It is not clear that any sort of large sample theory makes sense on a finite graph, but perhaps if the edge weights are drawn from probability distributions, then we might observe many weights sampled from these edges over time, and perhaps large sample theory could have some application over many observations of the edge weights.

Evaluating the quality of our weight estimates via a loss function as in (\ref{eq:loss_1}) makes sense insofar as a smaller loss presumably means our estimates are more useful, but this perspective misses the fact that our real goal is not to estimate the unobserved weights of $G$ accurately, but rather to accurately estimate the \textit{true shortest paths} on $G$ accurately. From this perspective, the precise estimated weight values are not at all relevant if the shortest path estimates they lead to are identical to the true shortest paths. After all, the costs paid when traversing a path on $G$ will reflect the true weight values, not their estimated values.

From this latter perspective, a good metric for performance evaluation would be concerned with comparing estimated shortest paths with the true shortest paths. However, it is not immediately clear how to measure the difference between an estimated shortest path from node $A$ to node $B$, and the true shortest path from node $A$ to node $B$. If the true shortest path from node $A$ to node $B$ involves the sequence of edges $(e_1, \dots, e_n)$, and the estimated shortest path from node $A$ to node $B$ involves the edges $(e'_1, \dots, e'_{n'})$, then perhaps one way to measure the difference between these paths would be with the loss

\[
    L(A, B) = \left(\sum_{i = 1}^n w(e_i) - \sum_{i = 1}^{n'} w(e'_i)\right)^2.
\]

In words, we compare the \textit{true} cost accrued along the \textit{true} shortest path from $A$ to $B$, with the \textit{true} cost accrued long the \textit{estimated} shortest path from $A$ to $B$.

However, note that since $\sum_{i = 1}^n w(e_i) \leq \sum_{i = 1}^{n'} w(e'_i)$ by definition of the shortest path, perhaps it is not necessary to square the difference between the costs of the two paths. Instead we could define

\[
    L(A, B) = \sum_{i = 1}^{n'} w(e'_i) - \sum_{i = 1}^n w(e_i).
\]

Given the above tentative measures of the quality of an estimated shortest path, perhaps the total loss over $G$ could be defined as

\[
    \text{Loss}(G) = \sum_{\substack{A, B \in V \\ A \neq B}} L(A, B).
\]

(Presumably $\text{Loss}(G)$ as defined above counts each path twice, since $L(A, B)$ and $L(B, A)$ will both be summands, so the criteria for summation above should be restricted to avoid such double counting. I am not sure how to enforce this condition under the summation symbol.)

% (When all the edge weights of $G$ are known, the problem of computing shortest paths on $G$ has been solved in several different ways \cite{clrs}, )

\subsection{Issues Encountered During Attempted Implementation}

A number of issues were encountered when attempting to implement some of the methods discussed above. These are listed below, not necessarily in chronological order.

The main theme in the problems encountered below is a lack of understanding of both the theory I attempted to implement (regarding both Gaussian processes in general, and graph Mat\'{e}rn Gaussian processes in particular), as well as of the tools I tried to use for the implementation (including various Python libraries such as NetworkX, TensorFlow, GPflow, OSMnx, etc.).

% -trouble with tools like networkx, gpflow, tensorflow
% -computer trouble with e.g. osmnx, conda, pip, Windows, Mac
% -related: trouble finding an appropriate dataset (openstreetmap, random graphs, doctorwho dataset)
% -not understanding what i was trying to implement, e.g. given some data, what exactly is the covariance function
% -lack of knowledge of how gaussian processes are used (related to above)
% -tensorflow tensor shape mismatch errors
\begin{enumerate}
    \item The main tool for working with graphs in Python appears to be NetworkX, which I had no familiarity with, so learning how to create and manipulate graphs (for example, adding weights to a graph) involved a lot of trial and error.
    \item I was intending to work with a dataset concerning traffic congestion, as in \cite{pmlr-v130-borovitskiy21a}. The graphs for traffic datasets appear to be typically acquired and manipulated using OSMnx, which according to the official documentation, should be installed with conda. However, the code supplied with the paper \cite{pmlr-v130-borovitskiy21a} is installed via pip, and I do not know how to work with both conda and pip. I initially began working with the code from \cite{pmlr-v130-borovitskiy21a} using pip, but then I tried to switch to conda to use OSMnx. However, I tried using conda to install the packages I had been using with pip, but conda was stuck on ``solving environment'' for about 36 hours before I decided to abort.
    \item Some of these issues may be related to the fact that I am borrowing a Windows laptop from the department, and it appears that Windows is generally not supported as well as macOS or Linux for the purposes of this project.
    \item Given the above issues, I decided to pivot and look for alternative datasets of weighted graphs. After some searching, I settled on a dataset concerning a British television show \cite{docwho}, because it seemed to avoid the use of OSMnx.
    \item However, while attempting to prepare the covariance matrix, I encountered ``tensor shape mismatch'' errors. I am not sure if this is due to my lack of understanding of the covariance matrix, or my lack of familiarity with NetworkX, TensorFlow and GPflow.
\end{enumerate}

Given the time limitations in this project, and the great temporal demands of another course I am taking this term, I was unable to actually implement a graph Mat\'{e}rn Gaussian process on a graph.

The issues described above are probably all rectificable, at least in principle, given enough time and perhaps enough support to become more knowledgeable about what I have been attempting to do during this project.

\subsection{Related Work}

Given the potential novelty of estimating shortest paths on an edge-weighted graph with partially unknown weights, it seems worthwhile to investigate whether any research has been done on this problem.

Searching Google for \textit{graph estimate shortest path} returns almost exclusively material concerned with the standard problem of computing shortest paths on an edge-weighted graph with known weights. The only relevant result was \cite{weiss2022generalization}, which seems to be more concerned with the notion that computing the edge weights of a graph may involve non-trivial computation, and therefore introduces the use of weight estimators to compute edge weights with increasing accuracy, at the cost of increased computation time.

Searching Google for \textit{graph shortest path weight unknown} again mostly returns material concerned with the standard shortest-path problem. The paper \cite{szepesvari2004shortest} is also concerned with edge weight computation. It is assumed that edge weights are initially unknown, but that there is a method to query the edge weights, and the goal is to minimize the number of queries. The paper \cite{tehrani2013distributed} considers an undirected graph whose edge weights are random variables with unknown distributions, but is concerned with distributed computing.

Lastly, apparently the problem of finding shortest paths on graphs where the graph structure itself is not entirely known has been studied \cite{papadimitriou1991shortest}; see also the \href{https://en.wikipedia.org/wiki/Canadian_traveller_problem#:~:text=In%20computer%20science%20and%20graph,a%20certain%20%22realization%20restriction.%22}{Canadian traveller problem} on Wikipedia.

In sum, it is not apparent that any prior work has explored the setting of an edge-weighted graph with only partially observed weights.

\subsection{Potential Applications}\label{sec:pot_app}

% another question: what types of graphs and datasets do you test on? Presumably estimating weights would work better when the weights have some kind of structuer the GP can identify, like traffic congestion.

The standard problem of finding shortest paths on an edge-weighted graph has many applications in navigation, logistics and computer network routing, among other areas. However, it is not immediately clear what kind of scenario involves a graph whose edge weights are only partially known. Perhaps we could imagine a robot intending to travel from one node on a graph to another while minimizing the cost accrued. The robot initially knows only some of the costs on the graph, and needs to estimate the other costs in order to plan its route. In this type of scenario, it probably makes sense that the costs are initially known only in a region of the graph near the robot, as opposed to any random subset of the graph. Perhaps as the robot travels along the graph, it observes more of the true costs on the graph's edges, and it updates its predictions as it observes more of the graph.

It is possible that the literature regarding the \href{https://en.wikipedia.org/wiki/Canadian_traveller_problem#:~:text=In%20computer%20science%20and%20graph,a%20certain%20%22realization%20restriction.%22}{Canadian traveller problem} might be a source of other possible applications.